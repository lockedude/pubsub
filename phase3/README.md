### Introduction
This application has been deployed entirely within AWS (Amazon Web Services) infrastructure. Please see the diagram in this directory (PubSub.pdf). To provide a summary, there are three (3) EC2 instances running in AWS, each of them running slightly differening versions of the code I have written in a docker container. One is running the pubsub_client, which implements publisher and subscriber functionality. Another is pubsub_server, which implements the subscribe and notify functionality, as well as the broker to the backend subscription tracker in DynamoDB. The third version of the code is pubsub_relay, which is similar to the pubsub_server, but has the added functionality of referring requests to other pubsub_servers in the event that some other subscriber exists that is not tracked by the initial referring pubsub_relay. The purpose of this third component was to implent filter-based routing within the pubsub system for phase 3 of the project. The pubsub client can make both subscription requests as well as publishing requests to the pubsub_server/relay. Once the pubsub_server/relay receives the subscribe event from a subscriber, it updates the record for that subscription in DynamoDB and adds the host that made the subscription request. Once the pubsub_server/relay receives a publish event, it checks DynamoDB for hosts that have subscribed to the subscription given in the publish event, and sends the message in the publish event to hosts found in DynamoDB. The pubsub_relay also accepts a relay event - that is when a pubsub_relay receives a publish event, it will take the parameters from the request and forward to other pubsub_relays using the relay event. The relay event is identical to the publish event, except that it does not also forward requests. This way only the initial pubsub_relay that received the publish event forwards requests to other pubsub_relays. 

### Architecture & Middleware
There are three EC2 instances running in AWS, each with their own container running. The use of the three EC2 instances was trivial as they could all just as easily been running on the same hosts in different containers, however I left the containers on separate hosts as the method in which the containers communicated did not depend on any local processing. Some firewall configuration was required to allow communication to occur on the designated TCP ports (49160, 8080 was used as the local port by docker). For EC2 to access other AWS services in my account I needed to configure an IAM role that was granted access to both DynamoDB as well as EC2 and assign that role to my respective instances. All requests between containers use HTTP POST requests to send notifications. The server running in the docker container is NodeJS, which allowed for easy event handling based on the incoming request. This was in part facilitated by the expressjs middleware, as well as the AWS SDK for interacting with DynamoDB. There was also a convenient library (http) for sending POST requests to other endpoints. 
